"""
Neural Network Training Script for Gearbox Parameter Prediction

This script trains a neural network model to predict gearbox parameters for a 
fixed two-stage gearbox based on input/output speeds and power requirements.

PREREQUISITE:
    Run data_generator.py first to generate the training data (data.csv).
    This CSV contains pre-validated gearbox parameter combinations.

INPUTS (What the model receives):
    - Input RPM (wp): Rotational speed at input shaft
    - Output RPM (wf): Desired rotational speed at output shaft  
    - Power (P): Power transmitted through the gearbox in horsepower (HP)

OUTPUTS (What the model predicts):
    - n1: Stage 1 input gear ratio
    - Pdn: Stage 1 Normal Diametral Pitch (teeth per inch)
    - Np1: Number of teeth in stage 1 pinion
    - Helix: Helix angle for stage 1 gears (degrees)
    - Pdn2: Stage 2 Normal Diametral Pitch (teeth per inch)
    - Np2: Number of teeth in stage 2 pinion
    - Helix2: Helix angle for stage 2 gears (degrees)

The model is trained on data generated by data_generator.py, which ensures
all training samples result in acceptable stress levels.
"""

# =============================================================================
# IMPORTS - Loading necessary libraries
# =============================================================================

import numpy as np                      # For numerical operations and arrays
import pandas as pd                     # For data handling and CSV operations
import tensorflow as tf                 # Main deep learning framework
from tensorflow import keras            # High-level neural network API
from sklearn.model_selection import train_test_split  # For splitting data
from sklearn.preprocessing import StandardScaler      # For normalizing data
from sklearn.metrics import r2_score                  # For R^2 calculation
import matplotlib.pyplot as plt         # For plotting training results
import itertools                        # For generating parameter combinations
import time                             # For tracking training time
import os                               # For file operations

# Import our existing gearbox calculation functions
import calculations as calc             # Contains stress calculation functions
import functions as fn                  # Contains utility functions like distance()

# =============================================================================
# CONFIGURATION - Setting up training parameters
# =============================================================================

# Random seed for reproducibility (makes results consistent across runs)
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# Training configuration
EPOCHS = 200                    # Number of times to iterate over the entire dataset (increased)
BATCH_SIZE = 131                # Number of samples processed before model update (increased for stability)
VALIDATION_SPLIT = 0.2          # Fraction of data to use for validation (20%)
LEARNING_RATE = 0.0005          # How quickly the model learns (reduced for better convergence)

# Model save path
MODEL_SAVE_PATH = "gearbox_nn_model.keras"  # Where to save the trained model
SCALER_SAVE_PATH = "gearbox_scaler.npz"     # Where to save the data scalers

# Allowable stress values (these are the target maximum stresses)
ALLOWABLE_BENDING_STRESS = 36.8403    # Maximum bending stress in ksi
ALLOWABLE_CONTACT_STRESS = 129.242     # Maximum contact stress in ksi

# =============================================================================
# STEP 1: LOAD DATA FROM CSV
# =============================================================================

def load_training_data_from_csv(csv_filename='data_genetic.csv'):
    """
    Load training data from the CSV file generated by data_generator.py.
    
    The CSV contains combinations of gearbox parameters that have been
    pre-validated to produce acceptable stress levels.
    
    CSV Structure (from data_generator.py):
    - wp: Input RPM
    - n1: Stage 1 gear ratio
    - Pnd: Stage 1 normal diametral pitch
    - Np1: Stage 1 pinion teeth
    - Helix: Stage 1 helix angle
    - Np2: Stage 2 pinion teeth
    - Pnd2: Stage 2 normal diametral pitch
    - Helix2: Stage 2 helix angle
    - Plus stress and metric columns (not used for training)
    
    Parameters:
        csv_filename (str): Path to the CSV file generated by data_generator.py
        
    Returns:
        tuple: (X_data, y_data) where X is inputs and y is outputs
    """
    
    print(f"Loading training data from {csv_filename}...")

    # Check if file exists
    if not os.path.exists(csv_filename):
        raise FileNotFoundError(
            f"CSV file '{csv_filename}' not found. "
            f"Please run data_generator.py first to generate the data."
        )

    # Load CSV file and normalize column names (strip whitespace/BOM)
    df = pd.read_csv(csv_filename)
    # Defensive: strip whitespace from column names
    df.columns = df.columns.str.strip()
    print(f"Loaded {len(df)} samples from CSV\n")

    # Required input and output column names (match columns in data.csv)
    input_cols = ['wp', 'wf', 'P']
    # data.csv uses Pd1/Pd2 and Helix1/Helix2 column names
    output_cols = ['n1', 'Pd1', 'Np1', 'Helix1', 'Pd2', 'Np2', 'Helix2']

    # Verify required columns exist
    missing_inputs = [c for c in input_cols if c not in df.columns]
    missing_outputs = [c for c in output_cols if c not in df.columns]
    if missing_inputs:
        raise KeyError(f"Missing input columns in CSV: {missing_inputs}")
    if missing_outputs:
        raise KeyError(f"Missing output columns in CSV: {missing_outputs}")

    # Extract arrays directly and ensure numeric dtype
    X_data = df[input_cols].astype(float).to_numpy()
    y_data = df[output_cols].astype(float).to_numpy()

    print(f"Successfully loaded {len(X_data)} training samples")
    print(f"Input features shape: {X_data.shape} (wp,wf,P)")
    print(f"Output parameters shape: {y_data.shape} (n1,Pnd,Np1,Helix,Pnd2,Np2,Helix2)\n")

    return X_data, y_data


# =============================================================================
# STEP 2: DATA PREPROCESSING
# =============================================================================

def preprocess_data(X_data, y_data):
    """
    Prepare the data for neural network training.
    
    Neural networks work best when:
    1. Data is split into training and validation sets
    2. Features are normalized (scaled to similar ranges)
    
    Parameters:
        X_data: Input features (RPM, power)
        y_data: Output parameters (gear parameters)
        
    Returns:
        tuple: (X_train, X_val, y_train, y_val, scaler_X, scaler_y)
    """
    
    print("Preprocessing data...")
    
    # Split data into training and validation sets
    # Training set: used to train the model
    # Validation set: used to check if model generalizes well to new data
    X_train, X_val, y_train, y_val = train_test_split(
        X_data, y_data, 
        test_size=VALIDATION_SPLIT,  # 20% for validation
        random_state=RANDOM_SEED
    )
    
    print(f"Training samples: {len(X_train)}")
    print(f"Validation samples: {len(X_val)}\n")
    
    # Normalize the data using StandardScaler
    # This transforms data to have mean=0 and standard deviation=1
    # This helps the neural network learn faster and more effectively
    
    # Create scalers for inputs and outputs
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    
    # Fit scalers on training data and transform both train and validation
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_val_scaled = scaler_X.transform(X_val)
    
    y_train_scaled = scaler_y.fit_transform(y_train)
    y_val_scaled = scaler_y.transform(y_val)
    
    print("Data normalized using StandardScaler")
    print(f"Input features - Mean: {scaler_X.mean_}, Std: {scaler_X.scale_}")
    print(f"Output parameters - Mean: {scaler_y.mean_}, Std: {scaler_y.scale_}\n")
    
    return X_train_scaled, X_val_scaled, y_train_scaled, y_val_scaled, scaler_X, scaler_y


# =============================================================================
# STEP 3: MODEL ARCHITECTURE
# =============================================================================

def build_model(input_dim, output_dim):
    """
    Build the neural network architecture.
    
    Architecture:
    - Input layer: receives the 3 input features (wp, wf, P)
    - Hidden layers: Multiple layers with ReLU activation to learn complex patterns
    - Output layer: Produces 7 output parameters
    - Dropout layers: Prevent overfitting by randomly dropping neurons during training
    
    Parameters:
        input_dim (int): Number of input features (3: wp, wf, P)
        output_dim (int): Number of output parameters (7: n1, Pdn1, Np1, etc.)
        
    Returns:
        keras.Model: Compiled neural network model
    """
    
    print("Building neural network model...")
    
    # Sequential model: layers are stacked one after another
    model = keras.Sequential([
        # Input layer - explicitly define input shape
        keras.layers.Input(shape=(input_dim,)),
        
        # First hidden layer: 256 neurons (increased capacity)
        # Dense = fully connected layer (each neuron connects to all previous neurons)
        # ReLU activation = max(0, x), introduces non-linearity
        keras.layers.Dense(256, name='hidden_layer_1'),
        keras.layers.BatchNormalization(name='batch_norm_1'),  # Normalize activations
        keras.layers.Activation('relu', name='activation_1'),
        keras.layers.Dropout(0.3, name='dropout_1'),  # Increased dropout for better regularization
        
        # Second hidden layer: 512 neurons (wider layer to capture complex patterns)
        keras.layers.Dense(512, name='hidden_layer_2'),
        keras.layers.BatchNormalization(name='batch_norm_2'),
        keras.layers.Activation('relu', name='activation_2'),
        keras.layers.Dropout(0.3, name='dropout_2'),
        
        # Third hidden layer: 256 neurons
        keras.layers.Dense(256, name='hidden_layer_3'),
        keras.layers.BatchNormalization(name='batch_norm_3'),
        keras.layers.Activation('relu', name='activation_3'),
        keras.layers.Dropout(0.3, name='dropout_3'),
        
        # Fourth hidden layer: 128 neurons (narrowing down)
        keras.layers.Dense(128, name='hidden_layer_4'),
        keras.layers.BatchNormalization(name='batch_norm_4'),
        keras.layers.Activation('relu', name='activation_4'),
        keras.layers.Dropout(0.2, name='dropout_4'),
        
        # Fifth hidden layer: 64 neurons (final feature extraction)
        keras.layers.Dense(64, name='hidden_layer_5'),
        keras.layers.BatchNormalization(name='batch_norm_5'),
        keras.layers.Activation('relu', name='activation_5'),
        
        # Output layer: produces 7 values (our gear parameters)
        # Linear activation (no activation function) for regression
        keras.layers.Dense(output_dim, activation='linear', name='output_layer')
    ])
    
    # Compile the model with:
    # - Optimizer: Adam (adaptive learning rate, works well for most problems)
    # - Loss function: MSE (Mean Squared Error) for regression
    # - Metrics: MAE (Mean Absolute Error) for easier interpretation
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='mse',           # Mean Squared Error
        metrics=['mae']       # Mean Absolute Error
    )
    
    # Print model summary to see the architecture
    print("\nModel Architecture:")
    model.summary()
    print()
    
    return model


# =============================================================================
# STEP 4: CUSTOM CALLBACK FOR STRESS VALIDATION
# =============================================================================

class StressValidationCallback(keras.callbacks.Callback):
    """
    Custom callback to validate predictions using stress calculations.
    
    This callback:
    1. Runs after each epoch
    2. Makes predictions on validation data
    3. Checks if predicted parameters result in acceptable stresses
    4. Reports the percentage of valid designs
    
    This ensures our model is learning to predict practical, usable designs.
    """
    
    def __init__(self, X_val, y_val, scaler_X, scaler_y):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.scaler_X = scaler_X
        self.scaler_y = scaler_y
    
    def _snap_to_valid(self, predicted_value, valid_options):
        """Helper to snap values to valid discrete options"""
        valid_options = np.array(valid_options)
        distances = np.abs(valid_options - predicted_value)
        closest_idx = np.argmin(distances)
        return valid_options[closest_idx]
    
    def on_epoch_end(self, epoch, logs=None):
        """Called at the end of each training epoch"""
        
        # Only validate every 10 epochs to save time
        if (epoch + 1) % 10 != 0:
            return
        
        # Make predictions on validation set
        y_pred_scaled = self.model.predict(self.X_val, verbose=0)
        
        # Convert predictions back to original scale
        y_pred = self.scaler_y.inverse_transform(y_pred_scaled)
        X_val_original = self.scaler_X.inverse_transform(self.X_val)
        
        # Count how many predictions result in valid designs
        valid_count = 0
        total_samples = min(100, len(y_pred))  # Check first 100 samples
        
        # Define valid discrete values
        VALID_PDN = [4, 5, 6, 8, 10]
        VALID_HELIX = [15, 20, 25]
        
        for i in range(total_samples):
            # Extract predicted parameters with proper constraints
            n1 = float(y_pred[i, 0])
            Pdn1 = int(self._snap_to_valid(y_pred[i, 1], VALID_PDN))
            Np1 = int(round(y_pred[i, 2]))
            Helix1 = int(self._snap_to_valid(y_pred[i, 3], VALID_HELIX))
            Pdn2 = int(self._snap_to_valid(y_pred[i, 4], VALID_PDN))
            Np2 = int(round(y_pred[i, 5]))
            Helix2 = int(self._snap_to_valid(y_pred[i, 6], VALID_HELIX))
            
            # Extract input conditions
            wp = float(X_val_original[i, 0])
            
            try:
                # Calculate stresses using predicted parameters
                sigma_b1 = calc.bending_stress(wp, n1, Pdn1, Np1, Helix1)
                sigma_c1 = calc.contact_stress(wp, n1, Pdn1, Np1, Helix1)
                
                # Get n2 for stage 2 calculations
                _, _, _, _, n2 = calc.important_values(wp, n1, Pdn1, Np1, Helix1)
                wf = wp / (n1 * n2)
                
                sigma_b2 = calc.bending_stress(wf, n2, Pdn2, Np2, Helix2)
                sigma_c2 = calc.contact_stress(wf, n2, Pdn2, Np2, Helix2)
                
                # Check if within allowable limits
                if (sigma_b1 < ALLOWABLE_BENDING_STRESS and 
                    sigma_c1 < ALLOWABLE_CONTACT_STRESS and
                    sigma_b2 < ALLOWABLE_BENDING_STRESS and 
                    sigma_c2 < ALLOWABLE_CONTACT_STRESS):
                    valid_count += 1
            except:
                # Invalid combination, skip
                continue
        
        # Report validation results
        valid_percentage = (valid_count / total_samples) * 100
        print(f"  â†’ Stress Validation: {valid_count}/{total_samples} " +
              f"predictions are valid ({valid_percentage:.1f}%)")


# =============================================================================
# STEP 5: TRAINING THE MODEL
# =============================================================================

def train_model(model, X_train, X_val, y_train, y_val, scaler_X, scaler_y):
    """
    Train the neural network model.
    
    Parameters:
        model: The compiled neural network
        X_train, X_val: Training and validation inputs
        y_train, y_val: Training and validation outputs
        scaler_X, scaler_y: Data scalers for inverse transformation
        
    Returns:
        history: Training history object containing loss and metrics
    """
    
    print("Starting model training...\n")
    
    # Define callbacks (functions called during training)
    callbacks = [
        # Early stopping: stop training if validation loss doesn't improve
        keras.callbacks.EarlyStopping(
            monitor='val_loss',      # Metric to monitor
            patience=25,             # Stop after 25 epochs without improvement (increased for better convergence)
            restore_best_weights=True,  # Load the best model weights
            verbose=1
        ),
        
        # Reduce learning rate when validation loss plateaus
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,              # Reduce learning rate by half
            patience=10,             # After 10 epochs without improvement (increased)
            min_lr=1e-7,             # Minimum learning rate
            verbose=1
        ),
        
        # Our custom stress validation callback
        StressValidationCallback(X_val, y_val, scaler_X, scaler_y)
    ]
    
    # Train the model
    # This is where the actual learning happens!
    history = model.fit(
        X_train, y_train,                    # Training data
        validation_data=(X_val, y_val),      # Validation data
        epochs=EPOCHS,                       # Maximum number of epochs
        batch_size=BATCH_SIZE,               # Samples per gradient update
        callbacks=callbacks,                 # Callbacks defined above
        verbose=1                            # Show progress bar
    )
    
    print("\nTraining completed!")
    
    return history


# =============================================================================
# STEP 6: EVALUATE AND VISUALIZE RESULTS
# =============================================================================

def evaluate_model(model, X_train, X_val, y_train, y_val, scaler_y, history):
    """
    Evaluate the trained model and visualize results.
    
    Parameters:
        model: Trained neural network
        X_train: Training inputs (scaled)
        X_val: Validation inputs (scaled)
        y_train: Training outputs (scaled)
        y_val: Validation outputs (scaled)
        scaler_y: Output scaler for inverse transformation
        history: Training history object
    """
    
    print("\n" + "="*80)
    print("MODEL EVALUATION")
    print("="*80 + "\n")
    
    # Make predictions on validation set
    y_pred_scaled = model.predict(X_val, verbose=0)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)
    y_val_original = scaler_y.inverse_transform(y_val)

    # Make predictions on training set to compute R^2
    y_train_pred_scaled = model.predict(X_train, verbose=0)
    y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled)
    y_train_original = scaler_y.inverse_transform(y_train)
    
    # Calculate metrics for each output parameter (match data.csv names)
    parameter_names = ['n1', 'Pd1', 'Np1', 'Helix1', 'Pd2', 'Np2', 'Helix2']
    
    print("Prediction Accuracy for Each Parameter:")
    print("-" * 80)
    r2_list = []
    for i, param_name in enumerate(parameter_names):
        # Calculate Mean Absolute Error (MAE) for this parameter
        mae = np.mean(np.abs(y_pred[:, i] - y_val_original[:, i]))
        # Calculate Mean Absolute Percentage Error (MAPE)
        mape = np.mean(np.abs((y_pred[:, i] - y_val_original[:, i]) / 
                              (y_val_original[:, i] + 1e-10))) * 100
        # Calculate R^2 on training data for this parameter
        try:
            r2_train = r2_score(y_train_original[:, i], y_train_pred[:, i])
        except Exception:
            r2_train = float('nan')
        r2_list.append(r2_train)
        print(f"{param_name:10s} - MAE: {mae:8.4f}, MAPE: {mape:6.2f}%, R2_train: {r2_train:6.3f}")
    
    print("\n" + "="*80 + "\n")
    
    # Plot training history
    plot_training_history(history)
    
    # Also print mean R^2 across parameters for training set
    mean_r2 = np.nanmean(r2_list) if len(r2_list) > 0 else float('nan')
    print(f"Mean training R^2 across parameters: {mean_r2:.3f}\n")

    # Show some example predictions
    show_example_predictions(y_val_original, y_pred, parameter_names, num_examples=5)


def plot_training_history(history):
    """
    Plot training and validation loss over epochs.
    
    Parameters:
        history: Training history object
    """
    
    plt.figure(figsize=(12, 4))
    
    # Plot loss
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss (MSE)')
    plt.title('Model Loss During Training')
    plt.legend()
    plt.grid(True)
    
    # Plot MAE
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='Training MAE')
    plt.plot(history.history['val_mae'], label='Validation MAE')
    plt.xlabel('Epoch')
    plt.ylabel('Mean Absolute Error')
    plt.title('Model MAE During Training')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
    print("Training history plot saved as: training_history.png")
    plt.close()


def show_example_predictions(y_true, y_pred, parameter_names, num_examples=5):
    """
    Display a few example predictions vs actual values.
    
    Parameters:
        y_true: True values
        y_pred: Predicted values
        parameter_names: Names of output parameters
        num_examples: Number of examples to show
    """
    
    print("\nExample Predictions:")
    print("="*80)
    
    for i in range(min(num_examples, len(y_true))):
        print(f"\nExample {i+1}:")
        print("-" * 80)
        print(f"{'Parameter':<10s} {'True Value':>12s} {'Predicted':>12s} {'Error':>12s}")
        print("-" * 80)
        
        for j, param_name in enumerate(parameter_names):
            true_val = y_true[i, j]
            pred_val = y_pred[i, j]
            error = pred_val - true_val
            
            print(f"{param_name:<10s} {true_val:12.2f} {pred_val:12.2f} {error:12.2f}")
    
    print("\n" + "="*80 + "\n")


# =============================================================================
# STEP 7: SAVE THE MODEL
# =============================================================================

def save_model_and_scalers(model, scaler_X, scaler_y):
    """
    Save the trained model and data scalers for future use.
    
    Parameters:
        model: Trained neural network
        scaler_X: Input scaler
        scaler_y: Output scaler
    """
    
    print("Saving model and scalers...")
    
    # Save the model in Keras format
    model.save(MODEL_SAVE_PATH)
    print(f"Model saved to: {MODEL_SAVE_PATH}")
    
    # Save scalers as numpy arrays
    np.savez(SCALER_SAVE_PATH,
             scaler_X_mean=scaler_X.mean_,
             scaler_X_scale=scaler_X.scale_,
             scaler_y_mean=scaler_y.mean_,
             scaler_y_scale=scaler_y.scale_)
    print(f"Scalers saved to: {SCALER_SAVE_PATH}")
    
    print("\nModel training complete! You can now use the model for predictions.\n")


# =============================================================================
# STEP 8: INFERENCE FUNCTION (Using the Trained Model)
# =============================================================================

def snap_to_valid_values(predicted_value, valid_options):
    """
    Snap a predicted continuous value to the nearest valid discrete option.
    
    For parameters that can only take specific discrete values (like Pdn or Helix),
    this function finds the closest valid option to the model's prediction.
    
    Parameters:
        predicted_value (float): The continuous value predicted by the model
        valid_options (list): List of valid discrete values
        
    Returns:
        The valid option closest to the predicted value
    """
    valid_options = np.array(valid_options)
    distances = np.abs(valid_options - predicted_value)
    closest_idx = np.argmin(distances)
    return valid_options[closest_idx]


def predict_gearbox_parameters(input_rpm, output_rpm, power_hp):
    """
    Use the trained model to predict gearbox parameters.
    
    Parameters:
        input_rpm (float): Input rotational speed in RPM
        output_rpm (float): Desired output rotational speed in RPM
        power_hp (float): Power in horsepower
        
    Returns:
        dict: Dictionary containing predicted gearbox parameters
    """
    
    # Load the saved model and scalers
    model = keras.models.load_model(MODEL_SAVE_PATH)
    
    scaler_data = np.load(SCALER_SAVE_PATH)
    scaler_X = StandardScaler()
    scaler_X.mean_ = scaler_data['scaler_X_mean']
    scaler_X.scale_ = scaler_data['scaler_X_scale']
    
    scaler_y = StandardScaler()
    scaler_y.mean_ = scaler_data['scaler_y_mean']
    scaler_y.scale_ = scaler_data['scaler_y_scale']
    
    # Prepare input vector: [wp, wf, P]
    X_input = np.array([[input_rpm, output_rpm, power_hp]], dtype=float)
    X_input_scaled = scaler_X.transform(X_input)
    
    # Make prediction
    y_pred_scaled = model.predict(X_input_scaled, verbose=0)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)[0]
    
    # Define valid discrete values for constrained parameters
    VALID_PDN = [4, 5, 6, 8, 10]      # Valid diametral pitch options
    VALID_HELIX = [15, 20, 25]         # Valid helix angle options (degrees)
    
    # Constrain discrete parameters to valid values
    # Return keys that match the training CSV column names
    parameters = {
        'n1': float(y_pred[0]),
        'Pd1': int(snap_to_valid_values(y_pred[1], VALID_PDN)),
        'Np1': int(round(y_pred[2])),
        'Helix1': int(snap_to_valid_values(y_pred[3], VALID_HELIX)),
        'Pd2': int(snap_to_valid_values(y_pred[4], VALID_PDN)),
        'Np2': int(round(y_pred[5])),
        'Helix2': int(snap_to_valid_values(y_pred[6], VALID_HELIX))
    }
    
    return parameters


# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """
    Main function that orchestrates the entire training process.
    """
    
    print("\n" + "="*80)
    print("NEURAL NETWORK TRAINING FOR GEARBOX PARAMETER PREDICTION")
    print("="*80 + "\n")
    
    start_time = time.time()
    
    # Step 1: Load training data from CSV
    # Data should be pre-generated using data_generator.py
    X_data, y_data = load_training_data_from_csv(csv_filename='data_genetic.csv')
    
    # Step 2: Preprocess the data
    X_train, X_val, y_train, y_val, scaler_X, scaler_y = preprocess_data(X_data, y_data)
    
    # Step 3: Build the model
    input_dim = X_train.shape[1]   # Number of input features (3)
    output_dim = y_train.shape[1]  # Number of output parameters (7)
    model = build_model(input_dim, output_dim)
    
    # Step 4: Train the model
    history = train_model(model, X_train, X_val, y_train, y_val, scaler_X, scaler_y)
    
    # Step 5: Evaluate the model (now includes training R^2)
    evaluate_model(model, X_train, X_val, y_train, y_val, scaler_y, history)
    
    # Step 6: Save the model
    save_model_and_scalers(model, scaler_X, scaler_y)
    
    # Calculate total training time
    elapsed_time = time.time() - start_time
    print(f"Total training time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)")
    
    # Step 7: Show example usage
    print("\n" + "="*80)
    print("EXAMPLE USAGE")
    print("="*80 + "\n")
    print("To use the trained model for prediction, run:")
    print("python train_gearbox_nn.py --predict")
    print("\nOr use the predict_gearbox_parameters() function in your code:")
    print("params = predict_gearbox_parameters(input_rpm=2000, output_rpm=250, power_hp=10)")
    print("\n" + "="*80 + "\n")


# =============================================================================
# COMMAND LINE INTERFACE
# =============================================================================

if __name__ == "__main__":
    import sys
    
    # Check if user wants to make a prediction
    if len(sys.argv) > 1 and sys.argv[1] == "--predict":
        # Interactive prediction mode
        print("\n" + "="*80)
        print("GEARBOX PARAMETER PREDICTION")
        print("="*80 + "\n")
        
        try:
            # Get user input
            input_rpm = float(input("Enter input RPM (1200-3600): "))
            output_rpm = float(input("Enter output RPM (100-500): "))
            power_hp = float(input("Enter power in HP (5-20): "))
            
            # Make prediction
            params = predict_gearbox_parameters(input_rpm, output_rpm, power_hp)
            
            # Display results
            print("\nPredicted Gearbox Parameters:")
            print("-" * 80)
            for key, value in params.items():
                print(f"{key:10s}: {value}")
            print("-" * 80 + "\n")
            
        except FileNotFoundError:
            print("\nError: Model not found. Please train the model first by running:")
            print("python train_gearbox_nn.py\n")
        except Exception as e:
            print(f"\nError: {e}\n")
    else:
        # Training mode (default)
        main()
