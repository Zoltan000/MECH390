"""
Neural Network Training Script for Gearbox Parameter Prediction

This script trains a neural network model to predict gearbox parameters for a 
fixed two-stage gearbox based on input/output speeds and power requirements.

PREREQUISITE:
    Run data_generator.py first to generate the training data (data.csv).
    This CSV contains pre-validated gearbox parameter combinations.

INPUTS (What the model receives):
    - Input RPM (wp): Rotational speed at input shaft
    - Output RPM (wf): Desired rotational speed at output shaft  
    - Power (P): Power transmitted through the gearbox in horsepower (HP)

OUTPUTS (What the model predicts):
    - n1: Stage 1 input gear ratio
    - Pdn: Stage 1 Normal Diametral Pitch (teeth per inch)
    - Np1: Number of teeth in stage 1 pinion
    - Helix: Helix angle for stage 1 gears (degrees)
    - Pdn2: Stage 2 Normal Diametral Pitch (teeth per inch)
    - Np2: Number of teeth in stage 2 pinion
    - Helix2: Helix angle for stage 2 gears (degrees)

The model is trained on data generated by data_generator.py, which ensures
all training samples result in acceptable stress levels.
"""

# =============================================================================
# IMPORTS - Loading necessary libraries
# =============================================================================

import numpy as np                      # For numerical operations and arrays
import pandas as pd                     # For data handling and CSV operations
import tensorflow as tf                 # Main deep learning framework
from tensorflow import keras            # High-level neural network API  # type: ignore
from sklearn.model_selection import train_test_split  # For splitting data
from sklearn.preprocessing import StandardScaler      # For normalizing data
import matplotlib.pyplot as plt         # For plotting training results
import itertools                        # For generating parameter combinations
import time                             # For tracking training time
import os                               # For file operations

# Import our existing gearbox calculation functions
import calculations as calc             # Contains stress calculation functions
import functions as fn                  # Contains utility functions like distance()

# =============================================================================
# CONFIGURATION - Setting up training parameters
# =============================================================================

# Random seed for reproducibility (makes results consistent across runs)
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# Training configuration
EPOCHS = 200                    # Number of times to iterate over the entire dataset (increased)
BATCH_SIZE = 131                # Number of samples processed before model update (increased for stability)
VALIDATION_SPLIT = 0.2          # Fraction of data to use for validation (20%)
LEARNING_RATE = 0.0005          # How quickly the model learns (reduced for better convergence)

# Model save path
MODEL_SAVE_PATH = "gearbox_nn_model.keras"  # Where to save the trained model
SCALER_SAVE_PATH = "gearbox_scaler.npz"     # Where to save the data scalers
VOLUME_MODEL_SAVE_PATH = "gearbox_volume_model.keras"
VOLUME_SCALER_SAVE_PATH = "gearbox_volume_scaler.npz"
PARAM_MODEL_SAVE_PATH = "gearbox_param_model.keras"
PARAM_SCALER_SAVE_PATH = "gearbox_param_scaler.npz"

# Allowable stress values (these are the target maximum stresses)
ALLOWABLE_BENDING_STRESS = 36.8403    # Maximum bending stress in ksi
ALLOWABLE_CONTACT_STRESS = 129.242     # Maximum contact stress in ksi

# =============================================================================
# STEP 1: LOAD DATA FROM CSV
# =============================================================================

def load_training_data_from_csv(csv_filename='data.csv'):
    """
    Load training data from the CSV file generated by data_generator.py.
    
    The CSV contains combinations of gearbox parameters that have been
    pre-validated to produce acceptable stress levels.
    
    CSV Structure (from data_generator.py):
    - wp: Input RPM
    - n1: Stage 1 gear ratio
    - Pnd: Stage 1 normal diametral pitch
    - Np1: Stage 1 pinion teeth
    - Helix: Stage 1 helix angle
    - Np2: Stage 2 pinion teeth
    - Pnd2: Stage 2 normal diametral pitch
    - Helix2: Stage 2 helix angle
    - Plus stress and metric columns (not used for training)
    
    Parameters:
        csv_filename (str): Path to the CSV file generated by data_generator.py
        
    Returns:
        tuple: (X_data, y_data) where X is inputs and y is outputs
    """
    
    print(f"Loading training data from {csv_filename}...")

    # Check if file exists
    if not os.path.exists(csv_filename):
        raise FileNotFoundError(
            f"CSV file '{csv_filename}' not found. "
            f"Please run data_generator.py first to generate the data."
        )

    # Load CSV file and normalize column names (strip whitespace/BOM)
    df = pd.read_csv(csv_filename)
    # Defensive: strip whitespace from column names
    df.columns = df.columns.str.strip()
    print(f"Loaded {len(df)} samples from CSV\n")

    # Required input and output column names
    input_cols = ['wp', 'wf', 'P']
    # rename Pnd/Pnd2 -> Pd1/Pd2
    output_cols = ['n1', 'Pd1', 'Np1', 'Helix', 'Pd2', 'Np2', 'Helix2']

    # Verify required columns exist
    missing_inputs = [c for c in input_cols if c not in df.columns]
    missing_outputs = [c for c in output_cols if c not in df.columns]
    if missing_inputs:
        raise KeyError(f"Missing input columns in CSV: {missing_inputs}")
    if missing_outputs:
        raise KeyError(f"Missing output columns in CSV: {missing_outputs}")

    # Extract arrays directly and ensure numeric dtype
    X_data = df[input_cols].astype(float).to_numpy()
    y_data = df[output_cols].astype(float).to_numpy()

    print(f"Successfully loaded {len(X_data)} training samples")
    print(f"Input features shape: {X_data.shape} (wp,wf,P)")
    print(f"Output parameters shape: {y_data.shape} (n1,Pd1,Np1,Helix,Pd2,Np2,Helix2)\n")

    return X_data, y_data


# =============================================================================
# STEP 2: DATA PREPROCESSING
# =============================================================================

def preprocess_data(X_data, y_data):
    """
    Prepare the data for neural network training.
    
    Neural networks work best when:
    1. Data is split into training and validation sets
    2. Features are normalized (scaled to similar ranges)
    
    Parameters:
        X_data: Input features (RPM, power)
        y_data: Output parameters (gear parameters)
        
    Returns:
        tuple: (X_train, X_val, y_train, y_val, scaler_X, scaler_y)
    """
    
    print("Preprocessing data...")
    
    # Split data into training and validation sets
    # Training set: used to train the model
    # Validation set: used to check if model generalizes well to new data
    X_train, X_val, y_train, y_val = train_test_split(
        X_data, y_data, 
        test_size=VALIDATION_SPLIT,  # 20% for validation
        random_state=RANDOM_SEED
    )
    
    print(f"Training samples: {len(X_train)}")
    print(f"Validation samples: {len(X_val)}\n")
    
    # Normalize the data using StandardScaler
    # This transforms data to have mean=0 and standard deviation=1
    # This helps the neural network learn faster and more effectively
    
    # Create scalers for inputs and outputs
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    
    # Fit scalers on training data and transform both train and validation
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_val_scaled = scaler_X.transform(X_val)
    
    y_train_scaled = scaler_y.fit_transform(y_train)
    y_val_scaled = scaler_y.transform(y_val)
    
    print("Data normalized using StandardScaler")
    print(f"Input features - Mean: {scaler_X.mean_}, Std: {scaler_X.scale_}")
    print(f"Output parameters - Mean: {scaler_y.mean_}, Std: {scaler_y.scale_}\n")
    
    return X_train_scaled, X_val_scaled, y_train_scaled, y_val_scaled, scaler_X, scaler_y


# =============================================================================
# STEP 3: MODEL ARCHITECTURE
# =============================================================================

def build_model(input_dim, output_dim):
    """
    Build the neural network architecture.
    
    Architecture:
    - Input layer: receives the 3 input features (wp, wf, P)
    - Hidden layers: Multiple layers with ReLU activation to learn complex patterns
    - Output layer: Produces 7 output parameters
    - Dropout layers: Prevent overfitting by randomly dropping neurons during training
    
    Parameters:
        input_dim (int): Number of input features (3: wp, wf, P)
        output_dim (int): Number of output parameters (7: n1, Pdn1, Np1, etc.)
        
    Returns:
        keras.Model: Compiled neural network model
    """
    
    print("Building neural network model...")
    
    # Sequential model: layers are stacked one after another
    model = keras.Sequential([
        # Input layer - explicitly define input shape
        keras.layers.Input(shape=(input_dim,)),
        
        # First hidden layer: 256 neurons (increased capacity)
        # Dense = fully connected layer (each neuron connects to all previous neurons)
        # ReLU activation = max(0, x), introduces non-linearity
        keras.layers.Dense(256, name='hidden_layer_1'),
        keras.layers.BatchNormalization(name='batch_norm_1'),  # Normalize activations
        keras.layers.Activation('relu', name='activation_1'),
        keras.layers.Dropout(0.3, name='dropout_1'),  # Increased dropout for better regularization
        
        # Second hidden layer: 512 neurons (wider layer to capture complex patterns)
        keras.layers.Dense(512, name='hidden_layer_2'),
        keras.layers.BatchNormalization(name='batch_norm_2'),
        keras.layers.Activation('relu', name='activation_2'),
        keras.layers.Dropout(0.3, name='dropout_2'),
        
        # Third hidden layer: 256 neurons
        keras.layers.Dense(256, name='hidden_layer_3'),
        keras.layers.BatchNormalization(name='batch_norm_3'),
        keras.layers.Activation('relu', name='activation_3'),
        keras.layers.Dropout(0.3, name='dropout_3'),
        
        # Fourth hidden layer: 128 neurons (narrowing down)
        keras.layers.Dense(128, name='hidden_layer_4'),
        keras.layers.BatchNormalization(name='batch_norm_4'),
        keras.layers.Activation('relu', name='activation_4'),
        keras.layers.Dropout(0.2, name='dropout_4'),
        
        # Fifth hidden layer: 64 neurons (final feature extraction)
        keras.layers.Dense(64, name='hidden_layer_5'),
        keras.layers.BatchNormalization(name='batch_norm_5'),
        keras.layers.Activation('relu', name='activation_5'),
        
        # Output layer: produces 7 values (our gear parameters)
        # Linear activation (no activation function) for regression
        keras.layers.Dense(output_dim, activation='linear', name='output_layer')
    ])
    
    # Compile the model with:
    # - Optimizer: Adam (adaptive learning rate, works well for most problems)
    # - Loss function: MSE (Mean Squared Error) for regression
    # - Metrics: MAE (Mean Absolute Error) for easier interpretation
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='mse',           # Mean Squared Error
        metrics=['mae']       # Mean Absolute Error
    )
    
    # Print model summary to see the architecture
    print("\nModel Architecture:")
    model.summary()
    print()
    
    return model


# =============================================================================
# STEP 4: CUSTOM CALLBACK FOR STRESS VALIDATION
# =============================================================================

class StressValidationCallback(keras.callbacks.Callback):
    """
    Custom callback to validate predictions using stress calculations.
    
    This callback:
    1. Runs after each epoch
    2. Makes predictions on validation data
    3. Checks if predicted parameters result in acceptable stresses
    4. Reports the percentage of valid designs
    
    This ensures our model is learning to predict practical, usable designs.
    """
    
    def __init__(self, X_val, y_val, scaler_X, scaler_y):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.scaler_X = scaler_X
        self.scaler_y = scaler_y
    
    def _snap_to_valid(self, predicted_value, valid_options):
        """Helper to snap values to valid discrete options"""
        valid_options = np.array(valid_options)
        distances = np.abs(valid_options - predicted_value)
        closest_idx = np.argmin(distances)
        return valid_options[closest_idx]
    
    def on_epoch_end(self, epoch, logs=None):
        """Called at the end of each training epoch"""
        
        # Only validate every 10 epochs to save time
        if (epoch + 1) % 10 != 0:
            return
        
        # Make predictions on validation set
        y_pred_scaled = self.model.predict(self.X_val, verbose=0)
        
        # Convert predictions back to original scale
        y_pred = self.scaler_y.inverse_transform(y_pred_scaled)
        X_val_original = self.scaler_X.inverse_transform(self.X_val)
        
        # Count how many predictions result in valid designs
        valid_count = 0
        total_samples = min(100, len(y_pred))  # Check first 100 samples
        
        # Define valid discrete values
        VALID_PD = [4, 5, 6, 8, 10]
        VALID_HELIX = [15, 20, 25]
        
        for i in range(total_samples):
            # Extract predicted parameters with proper constraints
            n1 = float(y_pred[i, 0])
            Pd1 = int(self._snap_to_valid(y_pred[i, 1], VALID_PD))
            Np1 = int(round(y_pred[i, 2]))
            Helix1 = int(self._snap_to_valid(y_pred[i, 3], VALID_HELIX))
            Pd2 = int(self._snap_to_valid(y_pred[i, 4], VALID_PD))
            Np2 = int(round(y_pred[i, 5]))
            Helix2 = int(self._snap_to_valid(y_pred[i, 6], VALID_HELIX))
            
            # Extract input conditions
            wp = float(X_val_original[i, 0])
            
            try:
                # Calculate stresses using the full results function
                res = calc.results(wp, n1, None, Pd1, Np1, Helix1, Pd2, Np2, Helix2, None, None, None)  # type: ignore[attr-defined]
                wf, P_calc, volume, sigma_b1, sigma_c1, sigma_b2, sigma_c2 = res  # type: ignore
                
                # Check if within allowable limits
                if (sigma_b1 < ALLOWABLE_BENDING_STRESS and 
                    sigma_c1 < ALLOWABLE_CONTACT_STRESS and
                    sigma_b2 < ALLOWABLE_BENDING_STRESS and 
                    sigma_c2 < ALLOWABLE_CONTACT_STRESS):
                    valid_count += 1
            except:
                # Invalid combination, skip
                continue
        
        # Report validation results
        valid_percentage = (valid_count / total_samples) * 100
        print(f"  â†’ Stress Validation: {valid_count}/{total_samples} " +
              f"predictions are valid ({valid_percentage:.1f}%)")


# =============================================================================
# STEP 5: TRAINING THE MODEL
# =============================================================================

def train_model(model, X_train, X_val, y_train, y_val, scaler_X, scaler_y):
    """
    Train the neural network model.
    
    Parameters:
        model: The compiled neural network
        X_train, X_val: Training and validation inputs
        y_train, y_val: Training and validation outputs
        scaler_X, scaler_y: Data scalers for inverse transformation
        
    Returns:
        history: Training history object containing loss and metrics
    """
    
    print("Starting model training...\n")
    
    # Define callbacks (functions called during training)
    callbacks = [
        # Early stopping: stop training if validation loss doesn't improve
        keras.callbacks.EarlyStopping(
            monitor='val_loss',      # Metric to monitor
            patience=25,             # Stop after 25 epochs without improvement (increased for better convergence)
            restore_best_weights=True,  # Load the best model weights
            verbose=1
        ),
        
        # Reduce learning rate when validation loss plateaus
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,              # Reduce learning rate by half
            patience=10,             # After 10 epochs without improvement (increased)
            min_lr=1e-7,             # Minimum learning rate
            verbose=1
        ),
        
        # Our custom stress validation callback
        StressValidationCallback(X_val, y_val, scaler_X, scaler_y)
    ]
    
    # Train the model
    # This is where the actual learning happens!
    history = model.fit(
        X_train, y_train,                    # Training data
        validation_data=(X_val, y_val),      # Validation data
        epochs=EPOCHS,                       # Maximum number of epochs
        batch_size=BATCH_SIZE,               # Samples per gradient update
        callbacks=callbacks,                 # Callbacks defined above
        verbose=1                            # Show progress bar
    )
    
    print("\nTraining completed!")
    
    return history


# =============================================================================
# STEP 6: EVALUATE AND VISUALIZE RESULTS
# =============================================================================

def evaluate_model(model, X_val, y_val, scaler_y, history):
    """
    Evaluate the trained model and visualize results.
    
    Parameters:
        model: Trained neural network
        X_val: Validation inputs
        y_val: Validation outputs
        scaler_y: Output scaler for inverse transformation
        history: Training history object
    """
    
    print("\n" + "="*80)
    print("MODEL EVALUATION")
    print("="*80 + "\n")
    
    # Make predictions on validation set
    y_pred_scaled = model.predict(X_val, verbose=0)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)
    y_val_original = scaler_y.inverse_transform(y_val)
    
    # Calculate metrics for each output parameter
    parameter_names = ['n1', 'Pd1', 'Np1', 'Helix', 'Pd2', 'Np2', 'Helix2']
    
    print("Prediction Accuracy for Each Parameter:")
    print("-" * 80)
    for i, param_name in enumerate(parameter_names):
        # Calculate Mean Absolute Error (MAE) for this parameter
        mae = np.mean(np.abs(y_pred[:, i] - y_val_original[:, i]))
        # Calculate Mean Absolute Percentage Error (MAPE)
        mape = np.mean(np.abs((y_pred[:, i] - y_val_original[:, i]) / 
                              (y_val_original[:, i] + 1e-10))) * 100
        print(f"{param_name:10s} - MAE: {mae:8.4f}, MAPE: {mape:6.2f}%")
    
    print("\n" + "="*80 + "\n")
    
    # Plot training history
    plot_training_history(history)
    
    # Show some example predictions
    show_example_predictions(y_val_original, y_pred, parameter_names, num_examples=5)


def plot_training_history(history):
    """
    Plot training and validation loss over epochs.
    
    Parameters:
        history: Training history object
    """
    
    plt.figure(figsize=(12, 4))
    
    # Plot loss
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss (MSE)')
    plt.title('Model Loss During Training')
    plt.legend()
    plt.grid(True)
    
    # Plot MAE
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='Training MAE')
    plt.plot(history.history['val_mae'], label='Validation MAE')
    plt.xlabel('Epoch')
    plt.ylabel('Mean Absolute Error')
    plt.title('Model MAE During Training')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
    print("Training history plot saved as: training_history.png")
    plt.close()


def show_example_predictions(y_true, y_pred, parameter_names, num_examples=5):
    """
    Display a few example predictions vs actual values.
    
    Parameters:
        y_true: True values
        y_pred: Predicted values
        parameter_names: Names of output parameters
        num_examples: Number of examples to show
    """
    
    print("\nExample Predictions:")
    print("="*80)
    
    for i in range(min(num_examples, len(y_true))):
        print(f"\nExample {i+1}:")
        print("-" * 80)
        print(f"{'Parameter':<10s} {'True Value':>12s} {'Predicted':>12s} {'Error':>12s}")
        print("-" * 80)
        
        for j, param_name in enumerate(parameter_names):
            true_val = y_true[i, j]
            pred_val = y_pred[i, j]
            error = pred_val - true_val
            
            print(f"{param_name:<10s} {true_val:12.2f} {pred_val:12.2f} {error:12.2f}")
    
    print("\n" + "="*80 + "\n")


# =============================================================================
# STEP 7: SAVE THE MODEL
# =============================================================================

def save_model_and_scalers(model, scaler_X, scaler_y):
    """
    Save the trained model and data scalers for future use.
    
    Parameters:
        model: Trained neural network
        scaler_X: Input scaler
        scaler_y: Output scaler
    """
    
    print("Saving model and scalers...")
    
    # Save the model in Keras format
    model.save(MODEL_SAVE_PATH)
    print(f"Model saved to: {MODEL_SAVE_PATH}")
    
    # Save scalers as numpy arrays
    np.savez(SCALER_SAVE_PATH,
             scaler_X_mean=scaler_X.mean_,
             scaler_X_scale=scaler_X.scale_,
             scaler_y_mean=scaler_y.mean_,
             scaler_y_scale=scaler_y.scale_)
    print(f"Scalers saved to: {SCALER_SAVE_PATH}")
    
    print("\nModel training complete! You can now use the model for predictions.\n")


def load_volume_data_from_csv(csv_filename='data_sample_10k.csv'):
    """Load input features and volume target from CSV.

    Returns X (N,3) and y (N,) arrays.
    """
    if not os.path.exists(csv_filename):
        raise FileNotFoundError(f"CSV file '{csv_filename}' not found.")
    df = pd.read_csv(csv_filename)
    df.columns = df.columns.str.strip()
    input_cols = ['wp', 'wf', 'P']
    if 'volume' not in df.columns:
        raise KeyError("CSV must contain a 'volume' column")
    X = df[input_cols].astype(float).to_numpy()
    y = df['volume'].astype(float).to_numpy()
    return X, y


def prepare_best_per_wp(csv_filename='data_sample_10k.csv'):
    """Prepare a dataset mapping (wp,wf,P) -> best-parameter-row (minimal volume per wp).

    Returns X (M,3) and y_params (M,7) where M is number of unique wp values in the CSV.
    """
    if not os.path.exists(csv_filename):
        raise FileNotFoundError(f"CSV file '{csv_filename}' not found.")
    df = pd.read_csv(csv_filename)
    df.columns = df.columns.str.strip()
    # Ensure required columns exist
    required = ['wp', 'wf', 'P', 'n1', 'Pd1', 'Np1', 'Helix', 'Pd2', 'Np2', 'Helix2', 'volume']
    for c in required:
        if c not in df.columns:
            raise KeyError(f"Missing required column in CSV: {c}")
    # Group by wp (wf and P are deterministic functions of wp in our generator)
    grouped = df.groupby('wp')
    rows = []
    for wp_val, group in grouped:
        # select row with minimal volume
        idx = group['volume'].astype(float).idxmin()
        rows.append(df.loc[idx])
    best_df = pd.DataFrame(rows)
    X = best_df[['wp', 'wf', 'P']].astype(float).to_numpy()
    y_params = best_df[['n1', 'Pd1', 'Np1', 'Helix', 'Pd2', 'Np2', 'Helix2']].astype(float).to_numpy()
    return X, y_params


def train_volume_model(csv_filename='data_sample_10k.csv', epochs=50, batch_size=64):
    """Train a model to predict minimal achievable volume given inputs (wp,wf,P).

    This trains a regression model mapping 3 inputs -> 1 output (volume).
    """
    print(f"Loading volume data from {csv_filename}...")
    X, y = load_volume_data_from_csv(csv_filename)
    # Split and scale
    X_train, X_val, y_train, y_val = train_test_split(X, y.reshape(-1, 1), test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED)
    scaler_X = StandardScaler(); scaler_y = StandardScaler()
    X_train_s = scaler_X.fit_transform(X_train); X_val_s = scaler_X.transform(X_val)
    y_train_s = scaler_y.fit_transform(y_train); y_val_s = scaler_y.transform(y_val)

    model = build_model(input_dim=X_train_s.shape[1], output_dim=1)
    history = model.fit(X_train_s, y_train_s, validation_data=(X_val_s, y_val_s), epochs=epochs, batch_size=batch_size, verbose=1)
    # Save model and scalers
    model.save(VOLUME_MODEL_SAVE_PATH)
    assert scaler_X.mean_ is not None and scaler_X.scale_ is not None
    assert scaler_y.mean_ is not None and scaler_y.scale_ is not None
    np.savez(VOLUME_SCALER_SAVE_PATH, scaler_X_mean=scaler_X.mean_, scaler_X_scale=scaler_X.scale_, scaler_y_mean=scaler_y.mean_, scaler_y_scale=scaler_y.scale_)
    print(f"Saved volume model to {VOLUME_MODEL_SAVE_PATH} and scalers to {VOLUME_SCALER_SAVE_PATH}")
    return model, scaler_X, scaler_y, history


def train_param_model(csv_filename='data_sample_10k.csv', epochs=100, batch_size=64):
    """Train a model to predict the parameter set that minimizes volume for each wp.

    Uses the best-per-wp rows as targets.
    """
    print(f"Preparing best-per-wp labels from {csv_filename}...")
    X, y_params = prepare_best_per_wp(csv_filename)
    X_train, X_val, y_train, y_val = train_test_split(X, y_params, test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED)
    scaler_X = StandardScaler(); scaler_y = StandardScaler()
    X_train_s = scaler_X.fit_transform(X_train); X_val_s = scaler_X.transform(X_val)
    y_train_s = scaler_y.fit_transform(y_train); y_val_s = scaler_y.transform(y_val)

    model = build_model(input_dim=X_train_s.shape[1], output_dim=y_train_s.shape[1])
    history = model.fit(X_train_s, y_train_s, validation_data=(X_val_s, y_val_s), epochs=epochs, batch_size=batch_size, verbose=1)
    model.save(PARAM_MODEL_SAVE_PATH)
    assert scaler_X.mean_ is not None and scaler_X.scale_ is not None
    assert scaler_y.mean_ is not None and scaler_y.scale_ is not None
    np.savez(PARAM_SCALER_SAVE_PATH, scaler_X_mean=scaler_X.mean_, scaler_X_scale=scaler_X.scale_, scaler_y_mean=scaler_y.mean_, scaler_y_scale=scaler_y.scale_)
    print(f"Saved param model to {PARAM_MODEL_SAVE_PATH} and scalers to {PARAM_SCALER_SAVE_PATH}")
    return model, scaler_X, scaler_y, history


def build_hybrid_model(input_dim):
    """Build a hybrid multi-head model:
    - shared base
    - params head (7 outputs, linear)
    - volume head (1 output, linear)
    - valid head (1 output, sigmoid)
    """
    inputs = keras.layers.Input(shape=(input_dim,))
    x = keras.layers.Dense(256)(inputs)
    x = keras.layers.BatchNormalization()(x)
    x = keras.layers.Activation('relu')(x)
    x = keras.layers.Dropout(0.3)(x)

    x = keras.layers.Dense(128)(x)
    x = keras.layers.BatchNormalization()(x)
    x = keras.layers.Activation('relu')(x)

    # Params head
    p = keras.layers.Dense(128)(x)
    p = keras.layers.Activation('relu')(p)
    params_out = keras.layers.Dense(7, activation='linear', name='params')(p)

    # Volume head
    v = keras.layers.Dense(64)(x)
    v = keras.layers.Activation('relu')(v)
    volume_out = keras.layers.Dense(1, activation='linear', name='volume')(v)

    # Validity head
    z = keras.layers.Dense(32)(x)
    z = keras.layers.Activation('relu')(z)
    valid_out = keras.layers.Dense(1, activation='sigmoid', name='valid')(z)

    model = keras.Model(inputs=inputs, outputs=[params_out, volume_out, valid_out])
    # Do not compile here; compilation (with loss_weights) happens at training time.
    model.summary()
    return model


def train_hybrid_model(csv_filename='data_sample_10k.csv', epochs=50, batch_size=64, loss_weights=None):
    """Train a hybrid model using best-per-wp rows as labels.

    X -> params (7), volume (1), valid (bool)
    """
    print(f"Preparing best-per-wp labels from {csv_filename} for hybrid training...")
    if not os.path.exists(csv_filename):
        raise FileNotFoundError(f"CSV file '{csv_filename}' not found.")
    df = pd.read_csv(csv_filename)
    df.columns = df.columns.str.strip()
    required = ['wp', 'wf', 'P', 'n1', 'Pd1', 'Np1', 'Helix', 'Pd2', 'Np2', 'Helix2', 'volume', 'valid']
    for c in required:
        if c not in df.columns:
            raise KeyError(f"Missing required column in CSV: {c}")

    # select best-per-wp (min volume) as training labels
    grouped = df.groupby('wp')
    rows = []
    for wp_val, group in grouped:
        idx = group['volume'].astype(float).idxmin()
        rows.append(df.loc[idx])
    best_df = pd.DataFrame(rows)

    X = best_df[['wp', 'wf', 'P']].astype(float).to_numpy()
    y_params = best_df[['n1', 'Pd1', 'Np1', 'Helix', 'Pd2', 'Np2', 'Helix2']].astype(float).to_numpy()
    y_volume = best_df[['volume']].astype(float).to_numpy()
    # Coerce valid column (may be string or bool) into integers 0/1
    y_valid = best_df['valid'].astype(str).str.strip().map({'True': 1, 'False': 0}).fillna(0).astype(int).to_numpy().reshape(-1, 1)

    # Split all arrays together to keep alignment
    X_train, X_val, yp_train, yp_val, yv_train, yv_val, val_train, val_val = train_test_split(
        X, y_params, y_volume, y_valid, test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED
    )

    scaler_X = StandardScaler(); scaler_y_params = StandardScaler(); scaler_y_vol = StandardScaler()
    X_train_s = scaler_X.fit_transform(X_train); X_val_s = scaler_X.transform(X_val)
    yp_train_s = scaler_y_params.fit_transform(yp_train); yp_val_s = scaler_y_params.transform(yp_val)
    yv_train_s = scaler_y_vol.fit_transform(yv_train); yv_val_s = scaler_y_vol.transform(yv_val)

    model = build_hybrid_model(input_dim=X_train_s.shape[1])

    # Default loss weights favour volume and validity more strongly
    if loss_weights is None:
        loss_weights = {'params': 1.0, 'volume': 2.0, 'valid': 5.0}

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
                  loss={'params': 'mse', 'volume': 'mse', 'valid': 'binary_crossentropy'},
                  loss_weights=loss_weights,
                  metrics={'params': 'mae', 'volume': 'mae', 'valid': 'accuracy'})

    history = model.fit(X_train_s, {'params': yp_train_s, 'volume': yv_train_s, 'valid': val_train},
                        validation_data=(X_val_s, {'params': yp_val_s, 'volume': yv_val_s, 'valid': val_val}),
                        epochs=epochs, batch_size=batch_size, verbose=1)

    # Save model and scalers
    model.save('gearbox_hybrid_model.keras')
    assert scaler_X.mean_ is not None and scaler_X.scale_ is not None
    assert scaler_y_params.mean_ is not None and scaler_y_params.scale_ is not None
    assert scaler_y_vol.mean_ is not None and scaler_y_vol.scale_ is not None
    np.savez('gearbox_hybrid_scalers.npz', scaler_X_mean=scaler_X.mean_, scaler_X_scale=scaler_X.scale_,
             scaler_y_params_mean=scaler_y_params.mean_, scaler_y_params_scale=scaler_y_params.scale_,
             scaler_y_vol_mean=scaler_y_vol.mean_, scaler_y_vol_scale=scaler_y_vol.scale_)

    print("Saved hybrid model to gearbox_hybrid_model.keras and scalers to gearbox_hybrid_scalers.npz")
    return model, scaler_X, scaler_y_params, scaler_y_vol, history


def predict_hybrid(input_rpm):
    """Load hybrid model and predict params+volume+valid for a given input_rpm."""
    scaler_data = np.load('gearbox_hybrid_scalers.npz')
    scaler_X = StandardScaler(); scaler_X.mean_ = scaler_data['scaler_X_mean']; scaler_X.scale_ = scaler_data['scaler_X_scale']
    scaler_y_params = StandardScaler(); scaler_y_params.mean_ = scaler_data['scaler_y_params_mean']; scaler_y_params.scale_ = scaler_data['scaler_y_params_scale']
    scaler_y_vol = StandardScaler(); scaler_y_vol.mean_ = scaler_data['scaler_y_vol_mean']; scaler_y_vol.scale_ = scaler_data['scaler_y_vol_scale']
    model = keras.models.load_model('gearbox_hybrid_model.keras')
    wp = float(input_rpm); wf = wp / 12 + 100; P = wp / 240
    X = np.array([[wp, wf, P]], dtype=float)
    Xs = scaler_X.transform(X)
    yp_s, yv_s, yvalid = model.predict(Xs, verbose=0)
    yp = scaler_y_params.inverse_transform(yp_s)
    yv = scaler_y_vol.inverse_transform(yv_s)
    # Build a raw parameter dict from continuous outputs (before snapping/search)
    raw = {
        'n1': float(yp[0, 0]),
        'Pd1': float(yp[0, 1]),
        'Np1': float(yp[0, 2]),
        'Helix1': float(yp[0, 3]),
        'Pd2': float(yp[0, 4]),
        'Np2': float(yp[0, 5]),
        'Helix2': float(yp[0, 6]),
        'volume_pred': float(yv[0, 0]),
        'valid_prob': float(yvalid[0, 0])
    }

    # Derived inputs
    wp = float(input_rpm); wf = wp / 12 + 100; P = wp / 240

    # Post-process to guarantee a valid design: snap discrete values and
    # perform a small local discrete search. If no valid candidate found,
    # fall back to best-per-wp from the dataset (guaranteed valid by generator).
    final = find_nearest_valid_design(wp, wf, P, raw)
    return final


def predict_min_volume(input_rpm):
    """Load volume model and predict minimal volume for given input_rpm."""
    scaler_data = np.load(VOLUME_SCALER_SAVE_PATH)
    scaler_X = StandardScaler(); scaler_X.mean_ = scaler_data['scaler_X_mean']; scaler_X.scale_ = scaler_data['scaler_X_scale']
    scaler_y = StandardScaler(); scaler_y.mean_ = scaler_data['scaler_y_mean']; scaler_y.scale_ = scaler_data['scaler_y_scale']
    model = keras.models.load_model(VOLUME_MODEL_SAVE_PATH)
    wp = float(input_rpm); wf = wp / 12 + 100; P = wp / 240
    X = np.array([[wp, wf, P]], dtype=float)
    Xs = scaler_X.transform(X)
    ys = model.predict(Xs, verbose=0)
    volume_pred = float(scaler_y.inverse_transform(ys.reshape(1, -1))[0, 0])
    return volume_pred


def predict_min_parameters(input_rpm):
    """Load param model and predict parameter set that minimizes volume for given input_rpm."""
    scaler_data = np.load(PARAM_SCALER_SAVE_PATH)
    scaler_X = StandardScaler(); scaler_X.mean_ = scaler_data['scaler_X_mean']; scaler_X.scale_ = scaler_data['scaler_X_scale']
    scaler_y = StandardScaler(); scaler_y.mean_ = scaler_data['scaler_y_mean']; scaler_y.scale_ = scaler_data['scaler_y_scale']
    model = keras.models.load_model(PARAM_MODEL_SAVE_PATH)
    wp = float(input_rpm); wf = wp / 12 + 100; P = wp / 240
    X = np.array([[wp, wf, P]], dtype=float)
    Xs = scaler_X.transform(X)
    ys = model.predict(Xs, verbose=0)
    y_inv = scaler_y.inverse_transform(ys)
    # Build dictionary of discrete-constrained parameters by snapping where appropriate
    VALID_PD = [4, 5, 6, 8, 10]
    VALID_HELIX = [15, 20, 25]
    n1 = float(y_inv[0, 0])
    Pd1 = int(snap_to_valid_values(y_inv[0, 1], VALID_PD))
    Np1 = int(round(y_inv[0, 2]))
    Helix1 = int(snap_to_valid_values(y_inv[0, 3], VALID_HELIX))
    Pd2 = int(snap_to_valid_values(y_inv[0, 4], VALID_PD))
    Np2 = int(round(y_inv[0, 5]))
    Helix2 = int(snap_to_valid_values(y_inv[0, 6], VALID_HELIX))
    return {
        'n1': n1, 'Pd1': Pd1, 'Np1': Np1, 'Helix1': Helix1,
        'Pd2': Pd2, 'Np2': Np2, 'Helix2': Helix2
    }


# =============================================================================
# STEP 8: INFERENCE FUNCTION (Using the Trained Model)
# =============================================================================

def snap_to_valid_values(predicted_value, valid_options):
    """
    Snap a predicted continuous value to the nearest valid discrete option.
    
    For parameters that can only take specific discrete values (like Pdn or Helix),
    this function finds the closest valid option to the model's prediction.
    
    Parameters:
        predicted_value (float): The continuous value predicted by the model
        valid_options (list): List of valid discrete values
        
    Returns:
        The valid option closest to the predicted value
    """
    valid_options = np.array(valid_options)
    distances = np.abs(valid_options - predicted_value)
    closest_idx = np.argmin(distances)
    return valid_options[closest_idx]


def get_domain_options():
    """Return discrete option lists for the search space.

    Tries to read `VARIABLES` from `data_generator` if available; otherwise
    falls back to hard-coded defaults matching the generator.
    """
    try:
        import data_generator as dg
        vars_map = getattr(dg, 'VARIABLES', None)
        if vars_map:
            n1_opts = list(np.array(vars_map.get('n1', np.arange(1, 9.1, 0.5))))
            Pd_opts = list(vars_map.get('Pd', [8, 10]))
            Np_opts = list(vars_map.get('Np1', range(10, 101, 5)))
            Helix_opts = list(vars_map.get('Helix', [15, 20, 25]))
            # Ensure Np options are ints
            Np_opts = [int(x) for x in Np_opts]
            return {
                'n1': n1_opts,
                'Pd': Pd_opts,
                'Np': Np_opts,
                'Helix': Helix_opts
            }
    except Exception:
        # If import fails, fall back to defaults
        pass

    return {
        'n1': list(np.arange(1, 9.1, 0.5)),
        'Pd': [8, 10],
        'Np': list(range(10, 101, 5)),
        'Helix': [15, 20, 25]
    }


def find_nearest_valid_design(wp, wf, P, raw_pred, max_radius=3, max_candidates=2000):
    """Attempt to find a valid discrete design near the model prediction.

    Strategy:
    - Snap continuous outputs to nearest discrete options
    - Search small neighbourhood (in discrete-index space) around snapped values
    - Evaluate candidates using `calc.results` and pick the valid one with
      minimal volume. If none found, fall back to `prepare_best_per_wp`.

    Returns a dict matching the format returned by prediction functions and
    guaranteed to have 'valid' == True.
    """
    domains = get_domain_options()

    # Helper to find index in a list of options
    def closest_idx(options, value):
        arr = np.array(options)
        idx = int(np.argmin(np.abs(arr - value)))
        return idx

    # Build center indices
    n1_opts = domains['n1']
    pd_opts = domains['Pd']
    np_opts = domains['Np']
    helix_opts = domains['Helix']

    center = {
        'n1': closest_idx(n1_opts, raw_pred['n1']),
        'Pd1': closest_idx(pd_opts, raw_pred['Pd1']),
        'Np1': closest_idx(np_opts, raw_pred['Np1']),
        'Helix1': closest_idx(helix_opts, raw_pred['Helix1']),
        'Pd2': closest_idx(pd_opts, raw_pred['Pd2']),
        'Np2': closest_idx(np_opts, raw_pred['Np2']),
        'Helix2': closest_idx(helix_opts, raw_pred['Helix2']),
    }

    # Prepare candidate index lists (small neighbourhoods)
    def idx_range(center_idx, max_r, upper_len):
        lo = max(0, center_idx - max_r)
        hi = min(upper_len - 1, center_idx + max_r)
        return list(range(lo, hi + 1))

    n1_idx_list = idx_range(center['n1'], max_radius, len(n1_opts))
    pd1_idx_list = idx_range(center['Pd1'], max_radius, len(pd_opts))
    np1_idx_list = idx_range(center['Np1'], max_radius, len(np_opts))
    helix1_idx_list = idx_range(center['Helix1'], max_radius, len(helix_opts))
    pd2_idx_list = idx_range(center['Pd2'], max_radius, len(pd_opts))
    np2_idx_list = idx_range(center['Np2'], max_radius, len(np_opts))
    helix2_idx_list = idx_range(center['Helix2'], max_radius, len(helix_opts))

    # Iterate candidates (bounded). Prioritise smaller neighbourhoods by breaking early
    best_candidate = None
    tried = 0

    for ni in n1_idx_list:
        for p1 in pd1_idx_list:
            for n1p in np1_idx_list:
                for h1 in helix1_idx_list:
                    for p2 in pd2_idx_list:
                        for n2p in np2_idx_list:
                            for h2 in helix2_idx_list:
                                tried += 1
                                if tried > max_candidates:
                                    break
                                # build candidate
                                cand_n1 = float(n1_opts[ni])
                                cand_Pd1 = int(pd_opts[p1])
                                cand_Np1 = int(np_opts[n1p])
                                cand_Helix1 = int(helix_opts[h1])
                                cand_Pd2 = int(pd_opts[p2])
                                cand_Np2 = int(np_opts[n2p])
                                cand_Helix2 = int(helix_opts[h2])

                                try:
                                    res = calc.results(wp, cand_n1, None,  # type: ignore[attr-defined]
                                                       cand_Pd1, cand_Np1, cand_Helix1,
                                                       cand_Pd2, cand_Np2, cand_Helix2,
                                                       None, None, None)
                                    wf_calc, P_calc, vol_calc, sb1, sc1, sb2, sc2 = res  # type: ignore
                                except Exception:
                                    continue
                                is_valid = (sb1 < ALLOWABLE_BENDING_STRESS and sc1 < ALLOWABLE_CONTACT_STRESS and
                                            sb2 < ALLOWABLE_BENDING_STRESS and sc2 < ALLOWABLE_CONTACT_STRESS)
                                if is_valid:
                                    # choose minimal volume
                                    if best_candidate is None or vol_calc < best_candidate['volume']:
                                        # attempt to get n2
                                        try:
                                            imp = calc.important_values(wp, cand_n1, cand_Pd1, cand_Np1, cand_Helix1)
                                            if isinstance(imp, (list, tuple)) and len(imp) >= 3:
                                                nd2 = float(imp[2])
                                            else:
                                                nd2 = 1.0
                                        except Exception:
                                            nd2 = 1.0
                                        best_candidate = {
                                            'n1': cand_n1,
                                            'Pd1': cand_Pd1,
                                            'Np1': cand_Np1,
                                            'Helix1': cand_Helix1,
                                            'Pd2': cand_Pd2,
                                            'Np2': cand_Np2,
                                            'Helix2': cand_Helix2,
                                            'volume': float(vol_calc),
                                            'sigma_b1': float(sb1),
                                            'sigma_c1': float(sc1),
                                            'sigma_b2': float(sb2),
                                            'sigma_c2': float(sc2),
                                            'wf': float(wf_calc),
                                            # 'n2' intentionally omitted from returned dict
                                            'valid': True
                                        }
        if tried > max_candidates:
            break

    if best_candidate is not None:
        return best_candidate

    # fallback: use best-per-wp from CSV if available (guaranteed valid)
    try:
        # Prefer the larger/newer sample CSV if present; fall back to older names.
        candidate_files = ['data_sample_50k.csv', 'data_sample_10k.csv', 'data.csv']
        found_file = None
        for f in candidate_files:
            if os.path.exists(f):
                found_file = f
                break

        if found_file is not None:
            Xb, yb = prepare_best_per_wp(found_file)
            # find row matching wp exactly
            import pandas as pd
            df = pd.read_csv(found_file)
            df.columns = df.columns.str.strip()
            sel = df[df['wp'].astype(float) == float(wp)]
            if not sel.empty:
                row = sel.iloc[0]
                return {
                    'n1': float(row['n1']),
                    'Pd1': int(row['Pd1']),
                    'Np1': int(row['Np1']),
                    'Helix1': int(row['Helix']),
                    'Pd2': int(row['Pd2']),
                    'Np2': int(row['Np2']),
                    'Helix2': int(row['Helix2']),
                    'volume': float(row['volume']),
                    'sigma_b1': float(row.get('sigma_bend_stage1', float('nan'))),
                    'sigma_c1': float(row.get('sigma_contact_stage1', float('nan'))),
                    'sigma_b2': float(row.get('sigma_bend_stage2', float('nan'))),
                    'sigma_c2': float(row.get('sigma_contact_stage2', float('nan'))),
                    'wf': float(row['wf']),
                    'valid': bool(row.get('valid', True))
                }
    except Exception:
        # If fallback lookup fails, continue to the raw_pred return below
        pass

    # As a last resort, return the raw prediction but mark valid False (should be rarely hit)
    # However per requirement we try to avoid this; returning best_df is preferred.
    return {
        'n1': float(raw_pred.get('n1', 1.0)),
        'Pd1': int(round(raw_pred.get('Pd1', 8))),
        'Np1': int(round(raw_pred.get('Np1', 20))),
        'Helix1': int(round(raw_pred.get('Helix1', 20))),
        'Pd2': int(round(raw_pred.get('Pd2', 8))),
        'Np2': int(round(raw_pred.get('Np2', 20))),
        'Helix2': int(round(raw_pred.get('Helix2', 20))),
        'volume': float(raw_pred.get('volume_pred', 1e9)),
        'sigma_b1': float('inf'),
        'sigma_c1': float('inf'),
        'sigma_b2': float('inf'),
        'sigma_c2': float('inf'),
        'wf': float(wf),
        'valid': False
    }


def predict_gearbox_parameters(input_rpm, output_rpm=None, power_hp=None, require_valid=False):
    """
    Use the trained model to predict gearbox parameters.

    Parameters:
        input_rpm (float): Input rotational speed in RPM
        output_rpm (float, optional): Desired output RPM. If not provided, it is
            derived from `input_rpm` using the same linear relation used in
            `calculations.important_values`: `wf = wp / 12 + 100`.
        power_hp (float, optional): Power in horsepower. If not provided, it is
            derived from `input_rpm` using `P = wp / 240`.

    Returns:
        dict: Dictionary containing predicted gearbox parameters
    """

    # Validate input RPM is within supported range
    if not (1200 <= float(input_rpm) <= 3600):
        raise ValueError("input_rpm must be between 1200 and 3600 RPM")

    # Derive missing values using the same linear relations as the calculations
    # module so predictions are consistent with training assumptions.
    if output_rpm is None:
        output_rpm = input_rpm / 12 + 100
    if power_hp is None:
        power_hp = input_rpm / 240

    # Load the saved model and scalers
    model = keras.models.load_model(MODEL_SAVE_PATH)

    scaler_data = np.load(SCALER_SAVE_PATH)
    scaler_X = StandardScaler()
    scaler_X.mean_ = scaler_data['scaler_X_mean']
    scaler_X.scale_ = scaler_data['scaler_X_scale']

    scaler_y = StandardScaler()
    scaler_y.mean_ = scaler_data['scaler_y_mean']
    scaler_y.scale_ = scaler_data['scaler_y_scale']

    # Prepare input vector: [wp, wf, P]
    X_input = np.array([[input_rpm, output_rpm, power_hp]], dtype=float)
    X_input_scaled = scaler_X.transform(X_input)
    
    # Make prediction
    y_pred_scaled = model.predict(X_input_scaled, verbose=0)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)[0]
    
    # Define valid discrete values for constrained parameters
    VALID_PD = [4, 5, 6, 8, 10]      # Valid diametral pitch options
    VALID_HELIX = [15, 20, 25]       # Valid helix angle options (degrees)

    # Constrain discrete parameters to valid values
    parameters = {
        'n1': float(y_pred[0]),
        'Pd1': int(snap_to_valid_values(y_pred[1], VALID_PD)),
        'Np1': int(round(y_pred[2])),
        'Helix1': int(snap_to_valid_values(y_pred[3], VALID_HELIX)),
        'Pd2': int(snap_to_valid_values(y_pred[4], VALID_PD)),
        'Np2': int(round(y_pred[5])),
        'Helix2': int(snap_to_valid_values(y_pred[6], VALID_HELIX))
    }

    # Validate predicted parameters by computing stresses using the calculations module
    wp = float(input_rpm)
    n1 = parameters['n1']
    Pd1 = parameters['Pd1']
    Np1 = parameters['Np1']
    Helix1 = parameters['Helix1']
    Pd2 = parameters['Pd2']
    Np2 = parameters['Np2']
    Helix2 = parameters['Helix2']

    # Use the high-level `results` helper from calculations to compute
    # wf, P, volume and stresses for stage 1 and stage 2. This ensures the
    # correct argument ordering and internal calculations are used.
    try:
        # `results` signature: results(wp, n1, n2, Pd1, Np1, Helix1, Pd2, Np2, Helix2, Pd3, Np3, Helix3)
        # For a 2-stage gearbox pass n2=None and None for stage-3 params.
        res = calc.results(wp, n1, None, Pd1, Np1, Helix1, Pd2, Np2, Helix2, None, None, None)  # type: ignore[attr-defined]
        # res returns (wf, P, Volume, Stage1BendingStress, Stage1ContactStress, Stage2BendingStress, Stage2ContactStress)
        wf = float(res[0])
        # P = res[1]  # not used here
        volume = float(res[2])
        sigma_b1 = float(res[3])
        sigma_c1 = float(res[4])
        sigma_b2 = float(res[5])
        sigma_c2 = float(res[6])

        # Try to get n2 (stage-2 ratio) using important_values which returns P, wf, nX
        try:
            imp = calc.important_values(wp, n1, None)
            if isinstance(imp, (list, tuple)) and len(imp) >= 3:
                n2 = float(imp[2])
            else:
                n2 = 1.0
        except Exception:
            n2 = 1.0
    except Exception:
        # If anything goes wrong, mark stresses as infinite so the prediction
        # is treated as invalid and the caller can handle it.
        sigma_b1 = sigma_c1 = sigma_b2 = sigma_c2 = float('inf')
        wf = input_rpm / 12 + 100
        n2 = 1.0

    # Determine validity
    valid = (sigma_b1 < ALLOWABLE_BENDING_STRESS and
             sigma_c1 < ALLOWABLE_CONTACT_STRESS and
             sigma_b2 < ALLOWABLE_BENDING_STRESS and
             sigma_c2 < ALLOWABLE_CONTACT_STRESS)

    # Attach validation info
    parameters.update({
        'sigma_b1': float(sigma_b1),
        'sigma_c1': float(sigma_c1),
        'sigma_b2': float(sigma_b2),
        'sigma_c2': float(sigma_c2),
        'valid': bool(valid),
        'wf': float(wf),
        'volume': float(volume)
    })

    # If the predicted set is invalid, perform a local discrete search to
    # find a nearby valid design; fall back to best-per-wp if necessary.
    if not valid:
        raw = {
            'n1': parameters['n1'],
            'Pd1': parameters['Pd1'],
            'Np1': parameters['Np1'],
            'Helix1': parameters['Helix1'],
            'Pd2': parameters['Pd2'],
            'Np2': parameters['Np2'],
            'Helix2': parameters['Helix2'],
            'volume_pred': parameters.get('volume', 1e9),
            'valid_prob': 0.0
        }
        found = find_nearest_valid_design(wp, wf, input_rpm / 240, raw)
        # Ensure found contains keys expected by caller
        return found

def main():
    """Main training function - trains the default gearbox model."""
    print("\n" + "="*80)
    print("NEURAL NETWORK TRAINING FOR GEARBOX PARAMETER PREDICTION")
    print("="*80 + "\n")
    
    start_time = time.time()
    
    # Step 1: Load training data from CSV
    # Try to find an available data file
    data_files = ['data.csv', 'data_sample_50k.csv', 'data_sample_10k.csv']
    csv_file = None
    for f in data_files:
        if os.path.exists(f):
            csv_file = f
            break
    
    if csv_file is None:
        raise FileNotFoundError(
            "No training data found. Please run data_generator.py first to generate data.csv, "
            "or ensure data_sample_50k.csv or data_sample_10k.csv exists."
        )
    
    X_data, y_data = load_training_data_from_csv(csv_filename=csv_file)
    
    # Step 2: Preprocess the data
    X_train, X_val, y_train, y_val, scaler_X, scaler_y = preprocess_data(X_data, y_data)
    
    # Step 3: Build the model
    input_dim = X_train.shape[1]   # Number of input features (3)
    output_dim = y_train.shape[1]  # Number of output parameters (7)
    model = build_model(input_dim, output_dim)
    
    # Step 4: Train the model
    history = train_model(model, X_train, X_val, y_train, y_val, scaler_X, scaler_y)
    
    # Step 5: Evaluate the model
    evaluate_model(model, X_val, y_val, scaler_y, history)
    
    # Step 6: Save the model
    save_model_and_scalers(model, scaler_X, scaler_y)
    
    # Calculate total training time
    elapsed_time = time.time() - start_time
    print(f"Total training time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)")
    
    # Step 7: Show example usage
    print("\n" + "="*80)
    print("EXAMPLE USAGE")
    print("="*80 + "\n")
    print("To use the trained model for prediction, run:")
    print("python train_gearbox_nn.py --predict")
    print("\nOr use the predict_gearbox_parameters() function in your code:")
    print("params = predict_gearbox_parameters(input_rpm=2000)  # wf and P derived from wp")
    print("\n" + "="*80 + "\n")


# =============================================================================
# COMMAND LINE INTERFACE
# =============================================================================

if __name__ == "__main__":
    import sys
    
    # Check if user wants to make a prediction
    if len(sys.argv) > 1 and sys.argv[1] == "--predict":
        # Interactive prediction mode
        print("\n" + "="*80)
        print("GEARBOX PARAMETER PREDICTION")
        print("="*80 + "\n")
        
        try:
            # Get user input (only input RPM is required; wf and P are derived)
            input_rpm = float(input("Enter input RPM (1200-3600): "))

            # Validate input range immediately for CLI clarity
            if not (1200 <= input_rpm <= 3600):
                raise ValueError("Input RPM must be between 1200 and 3600.")

            # Make prediction â€” use the hybrid predictor so CLI output matches
            # the JSON-style results returned by `predict_hybrid`.
            result = predict_hybrid(input_rpm)

            # Display results (print full dict keys/values to mirror JSON output)
            print("\nPredicted Gearbox Parameters:")
            print("-" * 80)
            for k, v in result.items():
                try:
                    print(f"{k:12s}: {v}")
                except Exception:
                    print(f"{k:12s}: {v}")
            print("-" * 80)

            # Show validation result if available
            if 'valid' in result and not result['valid']:
                print("\nWARNING: Predicted parameters exceed allowable stresses")
                print(f"  Stage1 bending: {result.get('sigma_b1'):.2f} ksi (allowable {ALLOWABLE_BENDING_STRESS})")
                print(f"  Stage1 contact: {result.get('sigma_c1'):.2f} ksi (allowable {ALLOWABLE_CONTACT_STRESS})")
                print(f"  Stage2 bending: {result.get('sigma_b2'):.2f} ksi (allowable {ALLOWABLE_BENDING_STRESS})")
                print(f"  Stage2 contact: {result.get('sigma_c2'):.2f} ksi (allowable {ALLOWABLE_CONTACT_STRESS})")
            else:
                print("\nAll predicted stresses are within allowable limits.")

            print()
            
        except FileNotFoundError:
            print("\nError: Model not found. Please train the model first by running:")
            print("python train_gearbox_nn.py\n")
        except Exception as e:
            print(f"\nError: {e}\n")
    else:
        # Training mode (default)
        # Extended CLI: support training/prediction for volume/params
        if len(sys.argv) > 1:
            cmd = sys.argv[1]
            if cmd == "--train-volume":
                csv = sys.argv[2] if len(sys.argv) > 2 else 'data_sample_10k.csv'
                train_volume_model(csv_filename=csv)
            elif cmd == "--train-params":
                csv = sys.argv[2] if len(sys.argv) > 2 else 'data_sample_10k.csv'
                train_param_model(csv_filename=csv)
            elif cmd == "--predict-volume":
                try:
                    input_rpm = float(input("Enter input RPM (1200-3600): "))
                    vol = predict_min_volume(input_rpm)
                    print(f"Predicted minimal volume: {vol:.3f} cubic inches")
                except Exception as e:
                    print(f"Error: {e}")
            elif cmd == "--predict-params":
                try:
                    input_rpm = float(input("Enter input RPM (1200-3600): "))
                    params = predict_min_parameters(input_rpm)
                    print("Predicted minimal-volume parameter set:")
                    for k, v in params.items():
                        print(f"  {k}: {v}")
                except Exception as e:
                    print(f"Error: {e}")
            elif cmd == "--train-hybrid":
                csv = sys.argv[2] if len(sys.argv) > 2 else 'data_sample_10k.csv'
                epochs = int(sys.argv[3]) if len(sys.argv) > 3 else 50
                train_hybrid_model(csv_filename=csv, epochs=epochs)
            elif cmd == "--predict-hybrid":
                try:
                    input_rpm = float(input("Enter input RPM (1200-3600): "))
                    result = predict_hybrid(input_rpm)
                    print("Hybrid prediction:")
                    for k, v in result.items():
                        print(f"  {k}: {v}")
                except Exception as e:
                    print(f"Error: {e}")
            else:
                main()
        else:
            main()
